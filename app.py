import os
import time
import io
import logging
import json
import warnings
import re
from flask import Flask, render_template, request, jsonify, Response, stream_with_context

warnings.filterwarnings("ignore")

# --- C·∫§U H√åNH CHO RENDER ---
chrome_bin_dir = "/opt/render/project/.render/chrome/opt/google/chrome"
if os.path.exists(chrome_bin_dir):
    os.environ["PATH"] += os.pathsep + chrome_bin_dir

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.core.os_manager import ChromeType
import google.generativeai as genai
from PIL import Image
from bs4 import BeautifulSoup

app = Flask(__name__, template_folder='templates')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def init_driver(user_agent=None):
    chrome_options = Options()
    chrome_options.add_argument("--headless=new") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080") # M√†n h√¨nh to ƒë·ªÉ load h·∫øt
    chrome_options.add_argument("--disable-extensions")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    
    if user_agent:
        chrome_options.add_argument(f'user-agent={user_agent}')

    chrome_binary_path = os.path.join(chrome_bin_dir, "google-chrome")
    if os.path.exists(chrome_binary_path):
        chrome_options.binary_location = chrome_binary_path

    try:
        service = Service(ChromeDriverManager(chrome_type=ChromeType.GOOGLE).install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        return driver
    except Exception as e:
        logger.error(f"‚ùå L·ªói Driver: {str(e)}")
        return None

def add_cookies(driver, cookie_str, url):
    if not cookie_str: return
    try:
        driver.get(url)
        time.sleep(2)
        cookies = cookie_str.split(';')
        for item in cookies:
            if '=' in item:
                parts = item.strip().split('=', 1)
                if len(parts) == 2:
                    name, value = parts
                    try: driver.add_cookie({'name': name, 'value': value})
                    except: pass
        driver.refresh()
        time.sleep(3)
    except Exception as e:
        logger.error(f"L·ªói n·∫°p cookie: {e}")

# --- TUY·ªÜT CHI√äU M·ªöI: L·∫§Y T·ª™ JSON ---
def extract_chapters_from_json(html_source):
    """T√¨m d·ªØ li·ªáu ng·∫ßm Next.js (__NEXT_DATA__)"""
    try:
        soup = BeautifulSoup(html_source, 'html.parser')
        script = soup.find('script', id='__NEXT_DATA__')
        if not script: return []

        data = json.loads(script.string)
        # C·∫•u tr√∫c th∆∞·ªùng g·∫∑p c·ªßa Metruyencv: 
        # props -> pageProps -> initialState -> story -> chapters
        # Ho·∫∑c props -> pageProps -> story -> chapters
        
        # T√¨m ƒë·ªá quy c√°c key c√≥ t√™n l√† 'chapters'
        def find_key(obj, key):
            if isinstance(obj, dict):
                if key in obj: return obj[key]
                for k, v in obj.items():
                    res = find_key(v, key)
                    if res: return res
            elif isinstance(obj, list):
                for item in obj:
                    res = find_key(item, key)
                    if res: return res
            return None

        chapters_data = find_key(data, 'chapters')
        
        results = []
        if chapters_data and isinstance(chapters_data, list):
            for c in chapters_data:
                # T√¨m ti√™u ƒë·ªÅ v√† slug/id
                title = c.get('name') or c.get('title') or f"Ch∆∞∆°ng {c.get('index')}"
                slug = c.get('slug') or c.get('id')
                # Gh√©p link
                url = f"https://metruyencv.com/truyen/{slug}" if slug else None
                # N·∫øu url ch∆∞a chu·∫©n, th·ª≠ gh√©p th·ªß c√¥ng (c·∫ßn s·ª≠a sau n·∫øu l·ªói)
                if url:
                    results.append({'title': title, 'url': url})
        
        return results
    except Exception as e:
        logger.error(f"L·ªói parse JSON: {e}")
        return []

@app.route('/get_chapters', methods=['POST'])
def get_chapters():
    data = request.json
    story_url = data.get('story_url')
    user_agent = data.get('user_agent')
    cookie_str = data.get('cookie_str')

    if not story_url: return jsonify({'error': 'Thi·∫øu Link!'})

    driver = init_driver(user_agent)
    if not driver: return jsonify({'error': 'L·ªói kh·ªüi t·∫°o Chrome.'})

    try:
        # 1. N·∫°p Cookie
        if cookie_str:
            from urllib.parse import urlparse
            domain = '{uri.scheme}://{uri.netloc}/'.format(uri=urlparse(story_url))
            add_cookies(driver, cookie_str, domain)

        logger.info(f"ƒêang v√†o: {story_url}")
        driver.get(story_url)
        time.sleep(5) 

        # 2. Check Cloudflare
        title = driver.title
        if "Just a moment" in title or "Cloudflare" in title:
            driver.quit()
            return jsonify({'error': f'üö® B·ªã Cloudflare ch·∫∑n! Title: {title}. H√£y check l·∫°i Cookie/User-Agent.'})

        # 3. C√ÅCH 1: L·∫§Y D·ªÆ LI·ªÜU NG·∫¶M (JSON) - Nhanh v√† chu·∫©n nh·∫•t
        html = driver.page_source
        chapters = extract_chapters_from_json(html)
        
        if len(chapters) > 0:
            logger.info(f"‚úÖ L·∫•y ƒë∆∞·ª£c {len(chapters)} ch∆∞∆°ng t·ª´ JSON ng·∫ßm.")
            driver.quit()
            return jsonify({'chapters': chapters, 'count': len(chapters)})

        # 4. C√ÅCH 2: N·∫æU JSON FAIL, D√ôNG SELENIUM CLICK (C·ªï ƒëi·ªÉn)
        logger.info("‚ö†Ô∏è Kh√¥ng th·∫•y JSON, chuy·ªÉn sang ch·∫ø ƒë·ªô Click th·ªß c√¥ng...")
        
        # Th·ª≠ click tab "Danh s√°ch ch∆∞∆°ng"
        try:
            driver.execute_script("""
                let tabs = document.querySelectorAll('a, button, div');
                for (let t of tabs) {
                    if (t.innerText && t.innerText.includes('Danh s√°ch ch∆∞∆°ng')) {
                        t.click();
                        break;
                    }
                }
            """)
            time.sleep(3)
        except: pass

        # Cu·ªôn xu·ªëng cu·ªëi
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)

        soup = BeautifulSoup(driver.page_source, 'html.parser')
        links = soup.find_all('a', href=True)
        
        seen = set()
        for link in links:
            href = link['href']
            txt = link.get_text(strip=True)
            if ('/chuong-' in href) and txt:
                if not href.startswith('http'): href = 'https://metruyencv.com' + href
                if href not in seen:
                    chapters.append({'title': txt, 'url': href})
                    seen.add(href)

        if len(chapters) == 0:
            # DEBUG MODE: Tr·∫£ v·ªÅ HTML ƒë·ªÉ anh hai bi·∫øt t·∫°i sao
            debug_html = soup.prettify()[:1000] # L·∫•y 1000 k√Ω t·ª± ƒë·∫ßu
            driver.quit()
            return jsonify({'error': f'V·∫´n t√¨m th·∫•y 0 ch∆∞∆°ng.\nTitle: {title}\nHTML (Debug): {debug_html}'})

        driver.quit()
        return jsonify({'chapters': chapters, 'count': len(chapters)})

    except Exception as e:
        if driver: driver.quit()
        return jsonify({'error': str(e)})

# --- H√ÄM C√ÄO 1 CH∆Ø∆†NG (Gi·ªØ nguy√™n ho·∫∑c t·ªëi ∆∞u nh·∫π) ---
def scrape_single_chapter_ocr(driver, url, model):
    try:
        driver.get(url)
        time.sleep(3)
        
        # Check ch·∫∑n
        if "Just a moment" in driver.title: return "[[B·ªä CH·∫∂N]]"

        # Ch·ª•p ·∫£nh (T·ªëi ƒëa 8 ·∫£nh ƒë·ªÉ ƒë·ª° lag)
        total_height = driver.execute_script("return document.body.scrollHeight")
        viewport = 1500
        images = []
        curr = 0
        while curr < total_height and len(images) < 8:
            driver.execute_script(f"window.scrollTo(0, {curr});")
            time.sleep(1)
            screenshot = driver.get_screenshot_as_png()
            images.append(Image.open(io.BytesIO(screenshot)).convert('RGB'))
            curr += viewport
        
        if not images: return "[[L·ªói ·∫£nh]]"

        # G·ª≠i AI
        text = ""
        batch = 3
        for i in range(0, len(images), batch):
            b_imgs = images[i:i+batch]
            try:
                res = model.generate_content(["OCR Ti·∫øng Vi·ªát. Ch·ªâ l·∫•y n·ªôi dung truy·ªán.", *b_imgs])
                text += res.text + "\n"
            except: pass
        return text
    except Exception as e:
        return f"[[L·ªói: {e}]]"

@app.route('/stream_scrape', methods=['POST'])
def stream_scrape():
    data = request.json
    urls = data.get('chapter_urls', [])
    ua = data.get('user_agent')
    ck = data.get('cookie_str')
    api_key = os.environ.get('GEMINI_API_KEY') or data.get('api_key')

    if not api_key: return jsonify({'error': 'Ch∆∞a c√≥ API Key'})

    def generate():
        driver = init_driver(ua)
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-flash') # D√πng b·∫£n Flash cho nhanh
        
        if ck and len(urls) > 0:
            from urllib.parse import urlparse
            d = '{uri.scheme}://{uri.netloc}/'.format(uri=urlparse(urls[0]))
            add_cookies(driver, ck, d)

        for i, url in enumerate(urls):
            yield json.dumps({'status': 'progress', 'msg': f'‚è≥ ƒêang x·ª≠ l√Ω ch∆∞∆°ng {i+1}/{len(urls)}...'}) + "\n"
            content = scrape_single_chapter_ocr(driver, url, model)
            
            yield json.dumps({
                'status': 'data',
                'chapter_index': i,
                'url': url,
                'content': content
            }) + "\n"
            time.sleep(2) 

        driver.quit()
        yield json.dumps({'status': 'done', 'msg': 'Xong!'}) + "\n"

    return Response(stream_with_context(generate()), mimetype='application/json')

@app.route('/')
def index():
    return render_template('index.html')

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
